{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "input_dim = 28 * 28\n",
    "inter_dim = 256\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=input_dim, inter_dim=inter_dim, latent_dim=latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, inter_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inter_dim, latent_dim * 2),\n",
    "            # Dont add ReLU anymore!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, inter_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inter_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        return mu + epsilon * torch.exp(logvar / 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        org_size = x.size()\n",
    "        batch = org_size[0]\n",
    "        x = x.view(batch, -1)\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=1)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        recon_x = self.decoder(z).view(size=org_size)\n",
    "\n",
    "        return recon_x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kl_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "recon_loss = lambda recon_x, x: F.binary_cross_entropy(recon_x, x, size_average=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_DATA/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af0ae94d6e349cd902ebfcbe098f9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_DATA/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_DATA/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060b4269a7dd4ede8b475b501d8cf9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_DATA/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_DATA/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990fa302d98e4214b48671389b2e7969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_DATA/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_DATA/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4f92496aec45a0b8f43266fdd677c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_DATA/MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "data_train = MNIST('MNIST_DATA/', train=True, download=True, transform=transform)\n",
    "data_valid = MNIST('MNIST_DATA/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(data_valid, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VAE(input_dim, inter_dim, latent_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enki\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss  548.912 \t Recon  548.890 \t KL  0.022 in Step 0\n",
      "Training loss  195.984 \t Recon  187.506 \t KL  8.478 in Step 100\n",
      "Training loss  186.497 \t Recon  180.251 \t KL  6.245 in Step 200\n",
      "Training loss  180.380 \t Recon  174.739 \t KL  5.641 in Step 300\n",
      "Training loss  173.424 \t Recon  167.742 \t KL  5.682 in Step 400\n",
      "Valid loss  174.210 \t Recon  168.798 \t KL  5.412 in epoch 0\n",
      "Model saved\n",
      "Epoch 1\n",
      "Training loss  176.966 \t Recon  171.667 \t KL  5.300 in Step 0\n",
      "Training loss  178.428 \t Recon  173.174 \t KL  5.254 in Step 100\n",
      "Training loss  164.144 \t Recon  158.866 \t KL  5.278 in Step 200\n",
      "Training loss  168.492 \t Recon  163.145 \t KL  5.348 in Step 300\n",
      "Training loss  167.551 \t Recon  162.487 \t KL  5.063 in Step 400\n",
      "Valid loss  167.272 \t Recon  162.062 \t KL  5.210 in epoch 1\n",
      "Model saved\n",
      "Epoch 2\n",
      "Training loss  165.251 \t Recon  160.121 \t KL  5.130 in Step 0\n",
      "Training loss  168.046 \t Recon  162.741 \t KL  5.305 in Step 100\n",
      "Training loss  160.912 \t Recon  156.062 \t KL  4.850 in Step 200\n",
      "Training loss  163.601 \t Recon  158.267 \t KL  5.334 in Step 300\n",
      "Training loss  164.387 \t Recon  159.275 \t KL  5.111 in Step 400\n",
      "Valid loss  164.483 \t Recon  159.270 \t KL  5.213 in epoch 2\n",
      "Model saved\n",
      "Epoch 3\n",
      "Training loss  162.315 \t Recon  157.274 \t KL  5.041 in Step 0\n",
      "Training loss  162.806 \t Recon  157.380 \t KL  5.426 in Step 100\n",
      "Training loss  167.607 \t Recon  162.258 \t KL  5.349 in Step 200\n",
      "Training loss  157.130 \t Recon  151.787 \t KL  5.343 in Step 300\n",
      "Training loss  158.434 \t Recon  153.097 \t KL  5.337 in Step 400\n",
      "Valid loss  161.917 \t Recon  156.514 \t KL  5.403 in epoch 3\n",
      "Model saved\n",
      "Epoch 4\n",
      "Training loss  162.329 \t Recon  157.147 \t KL  5.182 in Step 0\n",
      "Training loss  163.915 \t Recon  158.623 \t KL  5.293 in Step 100\n",
      "Training loss  158.314 \t Recon  152.849 \t KL  5.465 in Step 200\n",
      "Training loss  171.933 \t Recon  166.333 \t KL  5.600 in Step 300\n",
      "Training loss  159.332 \t Recon  153.887 \t KL  5.445 in Step 400\n",
      "Valid loss  160.006 \t Recon  154.492 \t KL  5.514 in epoch 4\n",
      "Model saved\n",
      "Epoch 5\n",
      "Training loss  154.826 \t Recon  149.297 \t KL  5.529 in Step 0\n",
      "Training loss  152.931 \t Recon  147.342 \t KL  5.590 in Step 100\n",
      "Training loss  158.774 \t Recon  153.461 \t KL  5.313 in Step 200\n",
      "Training loss  161.602 \t Recon  155.801 \t KL  5.800 in Step 300\n",
      "Training loss  154.167 \t Recon  148.740 \t KL  5.426 in Step 400\n",
      "Valid loss  158.954 \t Recon  153.212 \t KL  5.743 in epoch 5\n",
      "Model saved\n",
      "Epoch 6\n",
      "Training loss  157.174 \t Recon  151.229 \t KL  5.944 in Step 0\n",
      "Training loss  156.498 \t Recon  150.962 \t KL  5.536 in Step 100\n",
      "Training loss  158.854 \t Recon  153.513 \t KL  5.341 in Step 200\n",
      "Training loss  164.724 \t Recon  159.270 \t KL  5.455 in Step 300\n",
      "Training loss  163.486 \t Recon  158.109 \t KL  5.377 in Step 400\n",
      "Valid loss  157.553 \t Recon  152.011 \t KL  5.542 in epoch 6\n",
      "Model saved\n",
      "Epoch 7\n",
      "Training loss  158.873 \t Recon  153.228 \t KL  5.645 in Step 0\n",
      "Training loss  160.910 \t Recon  155.226 \t KL  5.684 in Step 100\n",
      "Training loss  158.151 \t Recon  152.806 \t KL  5.346 in Step 200\n",
      "Training loss  155.548 \t Recon  149.846 \t KL  5.703 in Step 300\n",
      "Training loss  159.322 \t Recon  153.865 \t KL  5.458 in Step 400\n",
      "Valid loss  157.043 \t Recon  151.258 \t KL  5.785 in epoch 7\n",
      "Model saved\n",
      "Epoch 8\n",
      "Training loss  151.899 \t Recon  146.048 \t KL  5.850 in Step 0\n",
      "Training loss  158.927 \t Recon  153.413 \t KL  5.514 in Step 100\n",
      "Training loss  154.847 \t Recon  149.223 \t KL  5.624 in Step 200\n",
      "Training loss  157.706 \t Recon  152.005 \t KL  5.702 in Step 300\n",
      "Training loss  152.465 \t Recon  146.802 \t KL  5.663 in Step 400\n",
      "Valid loss  156.454 \t Recon  150.793 \t KL  5.661 in epoch 8\n",
      "Model saved\n",
      "Epoch 9\n",
      "Training loss  159.732 \t Recon  154.033 \t KL  5.699 in Step 0\n",
      "Training loss  152.007 \t Recon  146.286 \t KL  5.720 in Step 100\n",
      "Training loss  160.514 \t Recon  154.612 \t KL  5.902 in Step 200\n",
      "Training loss  163.230 \t Recon  157.570 \t KL  5.660 in Step 300\n",
      "Training loss  153.825 \t Recon  148.381 \t KL  5.444 in Step 400\n",
      "Valid loss  155.635 \t Recon  149.988 \t KL  5.647 in epoch 9\n",
      "Model saved\n",
      "Epoch 10\n",
      "Training loss  160.211 \t Recon  154.709 \t KL  5.502 in Step 0\n",
      "Training loss  164.426 \t Recon  158.760 \t KL  5.665 in Step 100\n",
      "Training loss  156.276 \t Recon  150.399 \t KL  5.876 in Step 200\n",
      "Training loss  157.704 \t Recon  151.859 \t KL  5.844 in Step 300\n",
      "Training loss  155.294 \t Recon  149.602 \t KL  5.691 in Step 400\n",
      "Valid loss  155.416 \t Recon  149.750 \t KL  5.666 in epoch 10\n",
      "Model saved\n",
      "Epoch 11\n",
      "Training loss  148.885 \t Recon  142.885 \t KL  6.000 in Step 0\n",
      "Training loss  156.090 \t Recon  150.420 \t KL  5.670 in Step 100\n",
      "Training loss  145.902 \t Recon  140.213 \t KL  5.689 in Step 200\n",
      "Training loss  157.411 \t Recon  151.751 \t KL  5.660 in Step 300\n",
      "Training loss  155.877 \t Recon  150.102 \t KL  5.775 in Step 400\n",
      "Valid loss  154.745 \t Recon  148.828 \t KL  5.916 in epoch 11\n",
      "Model saved\n",
      "Epoch 12\n",
      "Training loss  158.067 \t Recon  152.045 \t KL  6.022 in Step 0\n",
      "Training loss  152.007 \t Recon  146.271 \t KL  5.736 in Step 100\n",
      "Training loss  156.218 \t Recon  150.497 \t KL  5.721 in Step 200\n",
      "Training loss  161.870 \t Recon  156.128 \t KL  5.742 in Step 300\n",
      "Training loss  153.553 \t Recon  147.931 \t KL  5.622 in Step 400\n",
      "Valid loss  154.566 \t Recon  148.806 \t KL  5.761 in epoch 12\n",
      "Model saved\n",
      "Epoch 13\n",
      "Training loss  152.278 \t Recon  146.351 \t KL  5.927 in Step 0\n",
      "Training loss  150.278 \t Recon  144.472 \t KL  5.806 in Step 100\n",
      "Training loss  156.431 \t Recon  150.700 \t KL  5.731 in Step 200\n",
      "Training loss  155.550 \t Recon  149.737 \t KL  5.813 in Step 300\n",
      "Training loss  154.860 \t Recon  148.961 \t KL  5.899 in Step 400\n",
      "Valid loss  153.944 \t Recon  148.286 \t KL  5.658 in epoch 13\n",
      "Model saved\n",
      "Epoch 14\n",
      "Training loss  148.619 \t Recon  142.863 \t KL  5.756 in Step 0\n",
      "Training loss  150.172 \t Recon  144.069 \t KL  6.102 in Step 100\n",
      "Training loss  158.190 \t Recon  152.431 \t KL  5.759 in Step 200\n",
      "Training loss  156.917 \t Recon  151.302 \t KL  5.615 in Step 300\n",
      "Training loss  151.529 \t Recon  145.524 \t KL  6.005 in Step 400\n",
      "Valid loss  153.777 \t Recon  148.194 \t KL  5.583 in epoch 14\n",
      "Model saved\n",
      "Epoch 15\n",
      "Training loss  153.830 \t Recon  148.328 \t KL  5.503 in Step 0\n",
      "Training loss  155.505 \t Recon  149.912 \t KL  5.593 in Step 100\n",
      "Training loss  151.865 \t Recon  145.897 \t KL  5.968 in Step 200\n",
      "Training loss  147.767 \t Recon  141.855 \t KL  5.912 in Step 300\n",
      "Training loss  150.418 \t Recon  144.563 \t KL  5.854 in Step 400\n",
      "Valid loss  153.407 \t Recon  147.602 \t KL  5.805 in epoch 15\n",
      "Model saved\n",
      "Epoch 16\n",
      "Training loss  149.897 \t Recon  144.018 \t KL  5.879 in Step 0\n",
      "Training loss  151.631 \t Recon  145.782 \t KL  5.848 in Step 100\n",
      "Training loss  147.754 \t Recon  141.983 \t KL  5.772 in Step 200\n",
      "Training loss  154.294 \t Recon  148.366 \t KL  5.928 in Step 300\n",
      "Training loss  158.319 \t Recon  152.743 \t KL  5.577 in Step 400\n",
      "Valid loss  153.311 \t Recon  147.487 \t KL  5.824 in epoch 16\n",
      "Model saved\n",
      "Epoch 17\n",
      "Training loss  154.910 \t Recon  149.293 \t KL  5.617 in Step 0\n",
      "Training loss  146.874 \t Recon  140.988 \t KL  5.886 in Step 100\n",
      "Training loss  153.529 \t Recon  147.922 \t KL  5.607 in Step 200\n",
      "Training loss  148.658 \t Recon  142.469 \t KL  6.188 in Step 300\n",
      "Training loss  151.067 \t Recon  145.103 \t KL  5.964 in Step 400\n",
      "Valid loss  153.099 \t Recon  147.225 \t KL  5.873 in epoch 17\n",
      "Model saved\n",
      "Epoch 18\n",
      "Training loss  142.969 \t Recon  136.903 \t KL  6.065 in Step 0\n",
      "Training loss  151.755 \t Recon  145.973 \t KL  5.782 in Step 100\n",
      "Training loss  145.669 \t Recon  139.820 \t KL  5.849 in Step 200\n",
      "Training loss  158.268 \t Recon  152.261 \t KL  6.007 in Step 300\n",
      "Training loss  151.165 \t Recon  145.040 \t KL  6.126 in Step 400\n",
      "Valid loss  153.064 \t Recon  147.289 \t KL  5.775 in epoch 18\n",
      "Model saved\n",
      "Epoch 19\n",
      "Training loss  151.732 \t Recon  146.102 \t KL  5.630 in Step 0\n",
      "Training loss  158.089 \t Recon  152.256 \t KL  5.834 in Step 100\n",
      "Training loss  155.939 \t Recon  150.256 \t KL  5.684 in Step 200\n",
      "Training loss  143.701 \t Recon  137.863 \t KL  5.838 in Step 300\n",
      "Training loss  153.793 \t Recon  147.852 \t KL  5.941 in Step 400\n",
      "Valid loss  152.573 \t Recon  146.665 \t KL  5.907 in epoch 19\n",
      "Model saved\n",
      "Epoch 20\n",
      "Training loss  153.979 \t Recon  147.969 \t KL  6.010 in Step 0\n",
      "Training loss  151.279 \t Recon  145.285 \t KL  5.994 in Step 100\n",
      "Training loss  154.223 \t Recon  148.421 \t KL  5.802 in Step 200\n",
      "Training loss  148.906 \t Recon  142.923 \t KL  5.983 in Step 300\n",
      "Training loss  153.511 \t Recon  147.563 \t KL  5.949 in Step 400\n",
      "Valid loss  152.374 \t Recon  146.557 \t KL  5.817 in epoch 20\n",
      "Model saved\n",
      "Epoch 21\n",
      "Training loss  150.824 \t Recon  145.074 \t KL  5.750 in Step 0\n",
      "Training loss  148.979 \t Recon  142.969 \t KL  6.010 in Step 100\n",
      "Training loss  149.621 \t Recon  143.807 \t KL  5.813 in Step 200\n",
      "Training loss  153.429 \t Recon  147.472 \t KL  5.957 in Step 300\n",
      "Training loss  142.472 \t Recon  136.394 \t KL  6.078 in Step 400\n",
      "Valid loss  152.171 \t Recon  146.152 \t KL  6.019 in epoch 21\n",
      "Model saved\n",
      "Epoch 22\n",
      "Training loss  146.582 \t Recon  140.543 \t KL  6.039 in Step 0\n",
      "Training loss  149.342 \t Recon  143.133 \t KL  6.209 in Step 100\n",
      "Training loss  145.062 \t Recon  139.184 \t KL  5.878 in Step 200\n",
      "Training loss  149.444 \t Recon  143.348 \t KL  6.096 in Step 300\n",
      "Training loss  152.211 \t Recon  146.253 \t KL  5.957 in Step 400\n",
      "Valid loss  152.083 \t Recon  145.990 \t KL  6.093 in epoch 22\n",
      "Model saved\n",
      "Epoch 23\n",
      "Training loss  152.327 \t Recon  146.160 \t KL  6.167 in Step 0\n",
      "Training loss  154.485 \t Recon  148.676 \t KL  5.809 in Step 100\n",
      "Training loss  153.239 \t Recon  147.225 \t KL  6.014 in Step 200\n",
      "Training loss  146.128 \t Recon  140.113 \t KL  6.015 in Step 300\n",
      "Training loss  155.114 \t Recon  149.257 \t KL  5.858 in Step 400\n",
      "Valid loss  152.163 \t Recon  146.345 \t KL  5.819 in epoch 23\n",
      "Epoch 24\n",
      "Training loss  150.309 \t Recon  144.418 \t KL  5.891 in Step 0\n",
      "Training loss  148.386 \t Recon  142.493 \t KL  5.894 in Step 100\n",
      "Training loss  153.072 \t Recon  147.031 \t KL  6.041 in Step 200\n",
      "Training loss  147.511 \t Recon  141.591 \t KL  5.919 in Step 300\n",
      "Training loss  152.677 \t Recon  146.574 \t KL  6.104 in Step 400\n",
      "Valid loss  151.572 \t Recon  145.659 \t KL  5.913 in epoch 24\n",
      "Model saved\n",
      "Epoch 25\n",
      "Training loss  153.471 \t Recon  147.648 \t KL  5.823 in Step 0\n",
      "Training loss  142.283 \t Recon  136.148 \t KL  6.135 in Step 100\n",
      "Training loss  145.563 \t Recon  139.517 \t KL  6.046 in Step 200\n",
      "Training loss  146.933 \t Recon  140.981 \t KL  5.952 in Step 300\n",
      "Training loss  150.867 \t Recon  144.756 \t KL  6.111 in Step 400\n",
      "Valid loss  151.468 \t Recon  145.515 \t KL  5.953 in epoch 25\n",
      "Model saved\n",
      "Epoch 26\n",
      "Training loss  147.829 \t Recon  142.057 \t KL  5.772 in Step 0\n",
      "Training loss  150.970 \t Recon  145.177 \t KL  5.793 in Step 100\n",
      "Training loss  153.791 \t Recon  147.888 \t KL  5.903 in Step 200\n",
      "Training loss  147.435 \t Recon  141.566 \t KL  5.869 in Step 300\n",
      "Training loss  155.354 \t Recon  149.559 \t KL  5.795 in Step 400\n",
      "Valid loss  151.403 \t Recon  145.416 \t KL  5.987 in epoch 26\n",
      "Model saved\n",
      "Epoch 27\n",
      "Training loss  149.752 \t Recon  143.538 \t KL  6.214 in Step 0\n",
      "Training loss  141.533 \t Recon  135.373 \t KL  6.159 in Step 100\n",
      "Training loss  151.668 \t Recon  145.650 \t KL  6.018 in Step 200\n",
      "Training loss  155.476 \t Recon  149.479 \t KL  5.997 in Step 300\n",
      "Training loss  143.455 \t Recon  137.574 \t KL  5.881 in Step 400\n",
      "Valid loss  151.430 \t Recon  145.388 \t KL  6.042 in epoch 27\n",
      "Epoch 28\n",
      "Training loss  153.332 \t Recon  147.339 \t KL  5.994 in Step 0\n",
      "Training loss  150.888 \t Recon  144.712 \t KL  6.176 in Step 100\n",
      "Training loss  157.206 \t Recon  151.074 \t KL  6.132 in Step 200\n",
      "Training loss  155.304 \t Recon  149.316 \t KL  5.988 in Step 300\n",
      "Training loss  150.604 \t Recon  144.604 \t KL  6.001 in Step 400\n",
      "Valid loss  151.187 \t Recon  145.110 \t KL  6.077 in epoch 28\n",
      "Model saved\n",
      "Epoch 29\n",
      "Training loss  147.219 \t Recon  141.080 \t KL  6.139 in Step 0\n",
      "Training loss  146.128 \t Recon  139.941 \t KL  6.187 in Step 100\n",
      "Training loss  160.246 \t Recon  154.585 \t KL  5.661 in Step 200\n",
      "Training loss  151.927 \t Recon  145.904 \t KL  6.023 in Step 300\n",
      "Training loss  151.350 \t Recon  145.405 \t KL  5.945 in Step 400\n",
      "Valid loss  150.886 \t Recon  144.988 \t KL  5.897 in epoch 29\n",
      "Model saved\n",
      "Epoch 30\n",
      "Training loss  154.650 \t Recon  148.704 \t KL  5.946 in Step 0\n",
      "Training loss  150.636 \t Recon  144.654 \t KL  5.982 in Step 100\n",
      "Training loss  157.102 \t Recon  151.046 \t KL  6.056 in Step 200\n",
      "Training loss  152.356 \t Recon  146.272 \t KL  6.084 in Step 300\n",
      "Training loss  146.614 \t Recon  140.564 \t KL  6.051 in Step 400\n",
      "Valid loss  151.089 \t Recon  145.146 \t KL  5.943 in epoch 30\n",
      "Epoch 31\n",
      "Training loss  153.495 \t Recon  147.608 \t KL  5.887 in Step 0\n",
      "Training loss  150.147 \t Recon  144.162 \t KL  5.985 in Step 100\n",
      "Training loss  147.910 \t Recon  141.794 \t KL  6.115 in Step 200\n",
      "Training loss  147.825 \t Recon  141.861 \t KL  5.964 in Step 300\n",
      "Training loss  143.916 \t Recon  137.644 \t KL  6.271 in Step 400\n",
      "Valid loss  150.506 \t Recon  144.364 \t KL  6.142 in epoch 31\n",
      "Model saved\n",
      "Epoch 32\n",
      "Training loss  153.458 \t Recon  147.489 \t KL  5.969 in Step 0\n",
      "Training loss  143.355 \t Recon  137.207 \t KL  6.148 in Step 100\n",
      "Training loss  144.635 \t Recon  138.366 \t KL  6.269 in Step 200\n",
      "Training loss  151.322 \t Recon  144.981 \t KL  6.341 in Step 300\n",
      "Training loss  149.969 \t Recon  143.985 \t KL  5.983 in Step 400\n",
      "Valid loss  150.575 \t Recon  144.467 \t KL  6.108 in epoch 32\n",
      "Epoch 33\n",
      "Training loss  138.343 \t Recon  132.197 \t KL  6.146 in Step 0\n",
      "Training loss  150.903 \t Recon  145.042 \t KL  5.861 in Step 100\n",
      "Training loss  148.589 \t Recon  142.538 \t KL  6.051 in Step 200\n",
      "Training loss  152.210 \t Recon  146.173 \t KL  6.038 in Step 300\n",
      "Training loss  148.313 \t Recon  142.230 \t KL  6.083 in Step 400\n",
      "Valid loss  150.548 \t Recon  144.616 \t KL  5.933 in epoch 33\n",
      "Epoch 34\n",
      "Training loss  141.646 \t Recon  135.632 \t KL  6.015 in Step 0\n",
      "Training loss  146.525 \t Recon  140.366 \t KL  6.158 in Step 100\n",
      "Training loss  146.034 \t Recon  139.620 \t KL  6.414 in Step 200\n",
      "Training loss  149.944 \t Recon  144.046 \t KL  5.898 in Step 300\n",
      "Training loss  152.914 \t Recon  146.876 \t KL  6.038 in Step 400\n",
      "Valid loss  150.355 \t Recon  144.275 \t KL  6.080 in epoch 34\n",
      "Model saved\n",
      "Epoch 35\n",
      "Training loss  145.507 \t Recon  139.402 \t KL  6.105 in Step 0\n",
      "Training loss  146.000 \t Recon  140.026 \t KL  5.974 in Step 100\n",
      "Training loss  146.335 \t Recon  140.150 \t KL  6.184 in Step 200\n",
      "Training loss  150.223 \t Recon  144.083 \t KL  6.139 in Step 300\n",
      "Training loss  148.951 \t Recon  142.868 \t KL  6.083 in Step 400\n",
      "Valid loss  150.715 \t Recon  144.631 \t KL  6.084 in epoch 35\n",
      "Epoch 36\n",
      "Training loss  150.095 \t Recon  144.003 \t KL  6.092 in Step 0\n",
      "Training loss  150.149 \t Recon  144.046 \t KL  6.103 in Step 100\n",
      "Training loss  141.239 \t Recon  134.850 \t KL  6.389 in Step 200\n",
      "Training loss  137.177 \t Recon  130.880 \t KL  6.297 in Step 300\n",
      "Training loss  149.782 \t Recon  143.910 \t KL  5.872 in Step 400\n",
      "Valid loss  150.148 \t Recon  144.002 \t KL  6.146 in epoch 36\n",
      "Model saved\n",
      "Epoch 37\n",
      "Training loss  139.195 \t Recon  133.005 \t KL  6.189 in Step 0\n",
      "Training loss  147.903 \t Recon  141.714 \t KL  6.189 in Step 100\n",
      "Training loss  145.443 \t Recon  139.249 \t KL  6.194 in Step 200\n",
      "Training loss  143.482 \t Recon  137.154 \t KL  6.328 in Step 300\n",
      "Training loss  141.496 \t Recon  135.358 \t KL  6.138 in Step 400\n",
      "Valid loss  150.338 \t Recon  144.233 \t KL  6.105 in epoch 37\n",
      "Epoch 38\n",
      "Training loss  138.997 \t Recon  132.761 \t KL  6.236 in Step 0\n",
      "Training loss  148.379 \t Recon  142.398 \t KL  5.981 in Step 100\n",
      "Training loss  144.140 \t Recon  137.888 \t KL  6.252 in Step 200\n",
      "Training loss  150.523 \t Recon  144.503 \t KL  6.020 in Step 300\n",
      "Training loss  148.990 \t Recon  142.829 \t KL  6.162 in Step 400\n",
      "Valid loss  150.169 \t Recon  144.089 \t KL  6.081 in epoch 38\n",
      "Epoch 39\n",
      "Training loss  148.766 \t Recon  142.618 \t KL  6.148 in Step 0\n",
      "Training loss  149.679 \t Recon  143.675 \t KL  6.004 in Step 100\n",
      "Training loss  151.845 \t Recon  145.408 \t KL  6.436 in Step 200\n",
      "Training loss  145.819 \t Recon  139.591 \t KL  6.228 in Step 300\n",
      "Training loss  151.848 \t Recon  145.692 \t KL  6.156 in Step 400\n",
      "Valid loss  150.224 \t Recon  144.001 \t KL  6.223 in epoch 39\n",
      "Epoch 40\n",
      "Training loss  151.885 \t Recon  145.735 \t KL  6.150 in Step 0\n",
      "Training loss  156.741 \t Recon  150.736 \t KL  6.005 in Step 100\n",
      "Training loss  145.377 \t Recon  139.501 \t KL  5.876 in Step 200\n",
      "Training loss  136.913 \t Recon  130.683 \t KL  6.230 in Step 300\n",
      "Training loss  148.094 \t Recon  141.976 \t KL  6.118 in Step 400\n",
      "Valid loss  150.117 \t Recon  143.961 \t KL  6.156 in epoch 40\n",
      "Model saved\n",
      "Epoch 41\n",
      "Training loss  142.948 \t Recon  136.576 \t KL  6.372 in Step 0\n",
      "Training loss  152.833 \t Recon  146.514 \t KL  6.319 in Step 100\n",
      "Training loss  149.563 \t Recon  143.526 \t KL  6.037 in Step 200\n",
      "Training loss  144.007 \t Recon  137.883 \t KL  6.124 in Step 300\n",
      "Training loss  146.264 \t Recon  139.830 \t KL  6.434 in Step 400\n",
      "Valid loss  149.971 \t Recon  143.895 \t KL  6.076 in epoch 41\n",
      "Model saved\n",
      "Epoch 42\n",
      "Training loss  147.740 \t Recon  141.581 \t KL  6.159 in Step 0\n",
      "Training loss  144.999 \t Recon  138.645 \t KL  6.354 in Step 100\n",
      "Training loss  146.356 \t Recon  140.247 \t KL  6.109 in Step 200\n",
      "Training loss  150.735 \t Recon  144.786 \t KL  5.949 in Step 300\n",
      "Training loss  144.949 \t Recon  138.835 \t KL  6.114 in Step 400\n",
      "Valid loss  150.220 \t Recon  144.108 \t KL  6.112 in epoch 42\n",
      "Epoch 43\n",
      "Training loss  141.959 \t Recon  135.818 \t KL  6.140 in Step 0\n",
      "Training loss  152.846 \t Recon  146.721 \t KL  6.125 in Step 100\n",
      "Training loss  145.123 \t Recon  138.964 \t KL  6.159 in Step 200\n",
      "Training loss  146.070 \t Recon  139.649 \t KL  6.421 in Step 300\n",
      "Training loss  149.067 \t Recon  142.790 \t KL  6.276 in Step 400\n",
      "Valid loss  149.803 \t Recon  143.604 \t KL  6.200 in epoch 43\n",
      "Model saved\n",
      "Epoch 44\n",
      "Training loss  145.895 \t Recon  139.690 \t KL  6.204 in Step 0\n",
      "Training loss  145.129 \t Recon  138.864 \t KL  6.265 in Step 100\n",
      "Training loss  153.462 \t Recon  147.189 \t KL  6.274 in Step 200\n",
      "Training loss  152.048 \t Recon  146.028 \t KL  6.019 in Step 300\n",
      "Training loss  142.875 \t Recon  136.681 \t KL  6.194 in Step 400\n",
      "Valid loss  149.854 \t Recon  143.653 \t KL  6.201 in epoch 44\n",
      "Epoch 45\n",
      "Training loss  142.212 \t Recon  135.821 \t KL  6.392 in Step 0\n",
      "Training loss  146.506 \t Recon  140.423 \t KL  6.083 in Step 100\n",
      "Training loss  147.954 \t Recon  141.984 \t KL  5.970 in Step 200\n",
      "Training loss  151.602 \t Recon  145.667 \t KL  5.934 in Step 300\n",
      "Training loss  142.775 \t Recon  136.445 \t KL  6.330 in Step 400\n",
      "Valid loss  149.730 \t Recon  143.616 \t KL  6.113 in epoch 45\n",
      "Model saved\n",
      "Epoch 46\n",
      "Training loss  143.667 \t Recon  137.415 \t KL  6.252 in Step 0\n",
      "Training loss  150.784 \t Recon  144.743 \t KL  6.040 in Step 100\n",
      "Training loss  150.281 \t Recon  143.912 \t KL  6.369 in Step 200\n",
      "Training loss  150.292 \t Recon  144.110 \t KL  6.182 in Step 300\n",
      "Training loss  149.342 \t Recon  143.021 \t KL  6.321 in Step 400\n",
      "Valid loss  149.978 \t Recon  143.833 \t KL  6.145 in epoch 46\n",
      "Epoch 47\n",
      "Training loss  144.858 \t Recon  138.677 \t KL  6.182 in Step 0\n",
      "Training loss  151.092 \t Recon  145.119 \t KL  5.973 in Step 100\n",
      "Training loss  146.931 \t Recon  140.573 \t KL  6.358 in Step 200\n",
      "Training loss  141.376 \t Recon  135.037 \t KL  6.339 in Step 300\n",
      "Training loss  148.123 \t Recon  141.947 \t KL  6.176 in Step 400\n",
      "Valid loss  149.795 \t Recon  143.711 \t KL  6.084 in epoch 47\n",
      "Epoch 48\n",
      "Training loss  147.645 \t Recon  141.550 \t KL  6.095 in Step 0\n",
      "Training loss  150.366 \t Recon  143.750 \t KL  6.615 in Step 100\n",
      "Training loss  150.634 \t Recon  144.396 \t KL  6.238 in Step 200\n",
      "Training loss  146.871 \t Recon  140.536 \t KL  6.334 in Step 300\n",
      "Training loss  147.421 \t Recon  141.303 \t KL  6.118 in Step 400\n",
      "Valid loss  149.981 \t Recon  143.728 \t KL  6.253 in epoch 48\n",
      "Epoch 49\n",
      "Training loss  142.752 \t Recon  136.409 \t KL  6.343 in Step 0\n",
      "Training loss  146.243 \t Recon  139.915 \t KL  6.327 in Step 100\n",
      "Training loss  154.434 \t Recon  148.047 \t KL  6.387 in Step 200\n",
      "Training loss  146.570 \t Recon  140.210 \t KL  6.360 in Step 300\n",
      "Training loss  147.776 \t Recon  141.575 \t KL  6.201 in Step 400\n",
      "Valid loss  149.601 \t Recon  143.414 \t KL  6.187 in epoch 49\n",
      "Model saved\n",
      "Epoch 50\n",
      "Training loss  138.763 \t Recon  132.404 \t KL  6.359 in Step 0\n",
      "Training loss  147.976 \t Recon  141.516 \t KL  6.460 in Step 100\n",
      "Training loss  143.662 \t Recon  137.339 \t KL  6.323 in Step 200\n",
      "Training loss  142.596 \t Recon  136.138 \t KL  6.458 in Step 300\n",
      "Training loss  142.059 \t Recon  135.886 \t KL  6.174 in Step 400\n",
      "Valid loss  149.670 \t Recon  143.503 \t KL  6.166 in epoch 50\n",
      "Epoch 51\n",
      "Training loss  144.119 \t Recon  138.142 \t KL  5.977 in Step 0\n",
      "Training loss  153.911 \t Recon  147.757 \t KL  6.154 in Step 100\n",
      "Training loss  149.719 \t Recon  143.502 \t KL  6.217 in Step 200\n",
      "Training loss  147.264 \t Recon  141.028 \t KL  6.236 in Step 300\n",
      "Training loss  139.806 \t Recon  133.472 \t KL  6.334 in Step 400\n",
      "Valid loss  149.509 \t Recon  143.278 \t KL  6.231 in epoch 51\n",
      "Model saved\n",
      "Epoch 52\n",
      "Training loss  147.732 \t Recon  141.509 \t KL  6.222 in Step 0\n",
      "Training loss  144.313 \t Recon  137.765 \t KL  6.548 in Step 100\n",
      "Training loss  155.333 \t Recon  149.238 \t KL  6.095 in Step 200\n",
      "Training loss  146.354 \t Recon  140.186 \t KL  6.168 in Step 300\n",
      "Training loss  147.198 \t Recon  140.868 \t KL  6.330 in Step 400\n",
      "Valid loss  149.514 \t Recon  143.291 \t KL  6.223 in epoch 52\n",
      "Epoch 53\n",
      "Training loss  152.333 \t Recon  145.975 \t KL  6.357 in Step 0\n",
      "Training loss  139.572 \t Recon  133.302 \t KL  6.270 in Step 100\n",
      "Training loss  148.169 \t Recon  142.032 \t KL  6.137 in Step 200\n",
      "Training loss  145.582 \t Recon  139.138 \t KL  6.444 in Step 300\n",
      "Training loss  150.588 \t Recon  144.256 \t KL  6.332 in Step 400\n",
      "Valid loss  149.500 \t Recon  143.380 \t KL  6.120 in epoch 53\n",
      "Model saved\n",
      "Epoch 54\n",
      "Training loss  153.358 \t Recon  147.311 \t KL  6.047 in Step 0\n",
      "Training loss  144.622 \t Recon  138.405 \t KL  6.217 in Step 100\n",
      "Training loss  144.627 \t Recon  138.492 \t KL  6.136 in Step 200\n",
      "Training loss  145.868 \t Recon  139.656 \t KL  6.212 in Step 300\n",
      "Training loss  142.864 \t Recon  136.935 \t KL  5.929 in Step 400\n",
      "Valid loss  149.475 \t Recon  143.111 \t KL  6.365 in epoch 54\n",
      "Model saved\n",
      "Epoch 55\n",
      "Training loss  145.201 \t Recon  138.778 \t KL  6.424 in Step 0\n",
      "Training loss  145.341 \t Recon  138.963 \t KL  6.378 in Step 100\n",
      "Training loss  150.160 \t Recon  144.063 \t KL  6.097 in Step 200\n",
      "Training loss  146.935 \t Recon  140.682 \t KL  6.253 in Step 300\n",
      "Training loss  147.252 \t Recon  141.285 \t KL  5.967 in Step 400\n",
      "Valid loss  149.302 \t Recon  143.150 \t KL  6.152 in epoch 55\n",
      "Model saved\n",
      "Epoch 56\n",
      "Training loss  153.121 \t Recon  147.093 \t KL  6.027 in Step 0\n",
      "Training loss  142.537 \t Recon  136.192 \t KL  6.345 in Step 100\n",
      "Training loss  157.017 \t Recon  150.780 \t KL  6.237 in Step 200\n",
      "Training loss  144.257 \t Recon  138.000 \t KL  6.257 in Step 300\n",
      "Training loss  142.750 \t Recon  136.440 \t KL  6.310 in Step 400\n",
      "Valid loss  149.493 \t Recon  143.178 \t KL  6.315 in epoch 56\n",
      "Epoch 57\n",
      "Training loss  137.446 \t Recon  130.929 \t KL  6.518 in Step 0\n",
      "Training loss  149.277 \t Recon  143.022 \t KL  6.255 in Step 100\n",
      "Training loss  143.246 \t Recon  136.940 \t KL  6.306 in Step 200\n",
      "Training loss  141.243 \t Recon  134.903 \t KL  6.340 in Step 300\n",
      "Training loss  149.025 \t Recon  142.586 \t KL  6.439 in Step 400\n",
      "Valid loss  149.178 \t Recon  143.041 \t KL  6.137 in epoch 57\n",
      "Model saved\n",
      "Epoch 58\n",
      "Training loss  141.011 \t Recon  134.889 \t KL  6.122 in Step 0\n",
      "Training loss  144.598 \t Recon  138.289 \t KL  6.309 in Step 100\n",
      "Training loss  148.819 \t Recon  142.498 \t KL  6.321 in Step 200\n",
      "Training loss  147.142 \t Recon  140.774 \t KL  6.368 in Step 300\n",
      "Training loss  147.237 \t Recon  140.839 \t KL  6.398 in Step 400\n",
      "Valid loss  149.281 \t Recon  143.106 \t KL  6.175 in epoch 58\n",
      "Epoch 59\n",
      "Training loss  145.636 \t Recon  139.420 \t KL  6.217 in Step 0\n",
      "Training loss  139.854 \t Recon  133.823 \t KL  6.031 in Step 100\n",
      "Training loss  150.537 \t Recon  144.365 \t KL  6.172 in Step 200\n",
      "Training loss  141.755 \t Recon  135.478 \t KL  6.277 in Step 300\n",
      "Training loss  141.933 \t Recon  135.786 \t KL  6.147 in Step 400\n",
      "Valid loss  149.313 \t Recon  143.051 \t KL  6.261 in epoch 59\n",
      "Epoch 60\n",
      "Training loss  150.820 \t Recon  144.388 \t KL  6.432 in Step 0\n",
      "Training loss  141.732 \t Recon  135.532 \t KL  6.200 in Step 100\n",
      "Training loss  137.022 \t Recon  130.594 \t KL  6.428 in Step 200\n",
      "Training loss  145.296 \t Recon  139.048 \t KL  6.249 in Step 300\n",
      "Training loss  147.542 \t Recon  141.323 \t KL  6.219 in Step 400\n",
      "Valid loss  149.483 \t Recon  143.316 \t KL  6.167 in epoch 60\n",
      "Epoch 61\n",
      "Training loss  145.821 \t Recon  139.631 \t KL  6.190 in Step 0\n",
      "Training loss  152.059 \t Recon  145.827 \t KL  6.233 in Step 100\n",
      "Training loss  151.158 \t Recon  144.972 \t KL  6.185 in Step 200\n",
      "Training loss  151.803 \t Recon  145.508 \t KL  6.294 in Step 300\n",
      "Training loss  148.531 \t Recon  142.041 \t KL  6.490 in Step 400\n",
      "Valid loss  149.324 \t Recon  142.886 \t KL  6.437 in epoch 61\n",
      "Epoch 62\n",
      "Training loss  150.815 \t Recon  144.347 \t KL  6.467 in Step 0\n",
      "Training loss  141.199 \t Recon  134.600 \t KL  6.599 in Step 100\n",
      "Training loss  154.625 \t Recon  148.389 \t KL  6.235 in Step 200\n",
      "Training loss  141.385 \t Recon  135.152 \t KL  6.233 in Step 300\n",
      "Training loss  145.196 \t Recon  138.668 \t KL  6.527 in Step 400\n",
      "Valid loss  149.102 \t Recon  142.810 \t KL  6.292 in epoch 62\n",
      "Model saved\n",
      "Epoch 63\n",
      "Training loss  146.886 \t Recon  140.701 \t KL  6.184 in Step 0\n",
      "Training loss  146.183 \t Recon  139.833 \t KL  6.350 in Step 100\n",
      "Training loss  149.322 \t Recon  143.223 \t KL  6.099 in Step 200\n",
      "Training loss  146.476 \t Recon  139.909 \t KL  6.567 in Step 300\n",
      "Training loss  145.051 \t Recon  138.908 \t KL  6.143 in Step 400\n",
      "Valid loss  149.226 \t Recon  143.016 \t KL  6.210 in epoch 63\n",
      "Epoch 64\n",
      "Training loss  139.099 \t Recon  132.876 \t KL  6.223 in Step 0\n",
      "Training loss  132.937 \t Recon  126.754 \t KL  6.184 in Step 100\n",
      "Training loss  138.528 \t Recon  132.254 \t KL  6.275 in Step 200\n",
      "Training loss  145.303 \t Recon  139.120 \t KL  6.184 in Step 300\n",
      "Training loss  148.005 \t Recon  141.699 \t KL  6.306 in Step 400\n",
      "Valid loss  149.237 \t Recon  143.052 \t KL  6.186 in epoch 64\n",
      "Epoch 65\n",
      "Training loss  153.775 \t Recon  147.615 \t KL  6.160 in Step 0\n",
      "Training loss  149.287 \t Recon  143.156 \t KL  6.130 in Step 100\n",
      "Training loss  151.530 \t Recon  145.358 \t KL  6.172 in Step 200\n",
      "Training loss  145.872 \t Recon  139.720 \t KL  6.153 in Step 300\n",
      "Training loss  147.680 \t Recon  141.353 \t KL  6.327 in Step 400\n",
      "Valid loss  149.065 \t Recon  142.815 \t KL  6.250 in epoch 65\n",
      "Model saved\n",
      "Epoch 66\n",
      "Training loss  143.764 \t Recon  137.474 \t KL  6.290 in Step 0\n",
      "Training loss  144.982 \t Recon  138.763 \t KL  6.219 in Step 100\n",
      "Training loss  143.365 \t Recon  136.917 \t KL  6.448 in Step 200\n",
      "Training loss  142.899 \t Recon  136.483 \t KL  6.416 in Step 300\n",
      "Training loss  146.796 \t Recon  140.473 \t KL  6.323 in Step 400\n",
      "Valid loss  149.372 \t Recon  143.041 \t KL  6.331 in epoch 66\n",
      "Epoch 67\n",
      "Training loss  150.976 \t Recon  144.730 \t KL  6.246 in Step 0\n",
      "Training loss  143.150 \t Recon  137.025 \t KL  6.124 in Step 100\n",
      "Training loss  145.155 \t Recon  139.132 \t KL  6.024 in Step 200\n",
      "Training loss  141.799 \t Recon  135.388 \t KL  6.411 in Step 300\n",
      "Training loss  142.840 \t Recon  136.516 \t KL  6.324 in Step 400\n",
      "Valid loss  149.344 \t Recon  142.922 \t KL  6.422 in epoch 67\n",
      "Epoch 68\n",
      "Training loss  150.879 \t Recon  144.504 \t KL  6.375 in Step 0\n",
      "Training loss  153.569 \t Recon  147.247 \t KL  6.321 in Step 100\n",
      "Training loss  145.280 \t Recon  138.756 \t KL  6.524 in Step 200\n",
      "Training loss  141.346 \t Recon  134.959 \t KL  6.388 in Step 300\n",
      "Training loss  150.330 \t Recon  144.229 \t KL  6.101 in Step 400\n",
      "Valid loss  148.949 \t Recon  142.748 \t KL  6.201 in epoch 68\n",
      "Model saved\n",
      "Epoch 69\n",
      "Training loss  147.520 \t Recon  141.421 \t KL  6.099 in Step 0\n",
      "Training loss  144.598 \t Recon  138.505 \t KL  6.093 in Step 100\n",
      "Training loss  147.856 \t Recon  141.546 \t KL  6.310 in Step 200\n",
      "Training loss  149.433 \t Recon  143.189 \t KL  6.245 in Step 300\n",
      "Training loss  147.575 \t Recon  141.450 \t KL  6.125 in Step 400\n",
      "Valid loss  148.987 \t Recon  142.734 \t KL  6.253 in epoch 69\n",
      "Epoch 70\n",
      "Training loss  146.196 \t Recon  140.018 \t KL  6.178 in Step 0\n",
      "Training loss  152.482 \t Recon  146.104 \t KL  6.378 in Step 100\n",
      "Training loss  139.650 \t Recon  133.226 \t KL  6.424 in Step 200\n",
      "Training loss  150.074 \t Recon  143.678 \t KL  6.396 in Step 300\n",
      "Training loss  140.954 \t Recon  134.819 \t KL  6.135 in Step 400\n",
      "Valid loss  149.012 \t Recon  142.708 \t KL  6.305 in epoch 70\n",
      "Epoch 71\n",
      "Training loss  144.640 \t Recon  138.338 \t KL  6.302 in Step 0\n",
      "Training loss  149.845 \t Recon  143.629 \t KL  6.216 in Step 100\n",
      "Training loss  139.745 \t Recon  133.299 \t KL  6.446 in Step 200\n",
      "Training loss  148.211 \t Recon  142.151 \t KL  6.060 in Step 300\n",
      "Training loss  150.445 \t Recon  144.314 \t KL  6.131 in Step 400\n",
      "Valid loss  149.283 \t Recon  143.090 \t KL  6.193 in epoch 71\n",
      "Epoch 72\n",
      "Training loss  149.998 \t Recon  143.711 \t KL  6.287 in Step 0\n",
      "Training loss  140.796 \t Recon  134.474 \t KL  6.322 in Step 100\n",
      "Training loss  143.859 \t Recon  137.564 \t KL  6.295 in Step 200\n",
      "Training loss  152.297 \t Recon  146.288 \t KL  6.008 in Step 300\n",
      "Training loss  144.869 \t Recon  138.491 \t KL  6.378 in Step 400\n",
      "Valid loss  149.048 \t Recon  142.713 \t KL  6.336 in epoch 72\n",
      "Epoch 73\n",
      "Training loss  152.046 \t Recon  145.729 \t KL  6.317 in Step 0\n",
      "Training loss  145.933 \t Recon  139.530 \t KL  6.402 in Step 100\n",
      "Training loss  141.174 \t Recon  134.827 \t KL  6.348 in Step 200\n",
      "Training loss  145.028 \t Recon  138.719 \t KL  6.309 in Step 300\n",
      "Training loss  147.568 \t Recon  141.367 \t KL  6.201 in Step 400\n",
      "Valid loss  149.259 \t Recon  142.904 \t KL  6.355 in epoch 73\n",
      "Epoch 74\n",
      "Training loss  155.081 \t Recon  148.688 \t KL  6.394 in Step 0\n",
      "Training loss  145.106 \t Recon  138.809 \t KL  6.298 in Step 100\n",
      "Training loss  135.589 \t Recon  129.038 \t KL  6.551 in Step 200\n",
      "Training loss  146.666 \t Recon  140.339 \t KL  6.327 in Step 300\n",
      "Training loss  145.769 \t Recon  139.547 \t KL  6.222 in Step 400\n",
      "Valid loss  149.113 \t Recon  142.918 \t KL  6.195 in epoch 74\n",
      "Epoch 75\n",
      "Training loss  152.224 \t Recon  145.942 \t KL  6.281 in Step 0\n",
      "Training loss  149.467 \t Recon  143.332 \t KL  6.135 in Step 100\n",
      "Training loss  136.839 \t Recon  130.351 \t KL  6.488 in Step 200\n",
      "Training loss  150.144 \t Recon  144.103 \t KL  6.041 in Step 300\n",
      "Training loss  149.148 \t Recon  142.953 \t KL  6.195 in Step 400\n",
      "Valid loss  149.103 \t Recon  142.931 \t KL  6.172 in epoch 75\n",
      "Epoch 76\n",
      "Training loss  142.227 \t Recon  135.938 \t KL  6.289 in Step 0\n",
      "Training loss  152.383 \t Recon  146.114 \t KL  6.270 in Step 100\n",
      "Training loss  146.627 \t Recon  140.305 \t KL  6.321 in Step 200\n",
      "Training loss  142.013 \t Recon  135.579 \t KL  6.434 in Step 300\n",
      "Training loss  147.641 \t Recon  141.408 \t KL  6.233 in Step 400\n",
      "Valid loss  149.119 \t Recon  142.839 \t KL  6.279 in epoch 76\n",
      "Epoch 77\n",
      "Training loss  142.449 \t Recon  135.848 \t KL  6.601 in Step 0\n",
      "Training loss  152.098 \t Recon  145.786 \t KL  6.312 in Step 100\n",
      "Training loss  148.593 \t Recon  142.094 \t KL  6.499 in Step 200\n",
      "Training loss  148.820 \t Recon  142.514 \t KL  6.306 in Step 300\n",
      "Training loss  145.458 \t Recon  139.110 \t KL  6.348 in Step 400\n",
      "Valid loss  149.041 \t Recon  142.822 \t KL  6.220 in epoch 77\n",
      "Epoch 78\n",
      "Training loss  141.627 \t Recon  135.383 \t KL  6.244 in Step 0\n",
      "Training loss  145.241 \t Recon  138.958 \t KL  6.282 in Step 100\n",
      "Training loss  148.817 \t Recon  142.493 \t KL  6.324 in Step 200\n",
      "Training loss  146.754 \t Recon  140.357 \t KL  6.397 in Step 300\n",
      "Training loss  141.333 \t Recon  134.669 \t KL  6.664 in Step 400\n",
      "Valid loss  149.219 \t Recon  142.911 \t KL  6.308 in epoch 78\n",
      "Epoch 79\n",
      "Training loss  140.466 \t Recon  134.259 \t KL  6.206 in Step 0\n",
      "Training loss  147.806 \t Recon  141.539 \t KL  6.267 in Step 100\n",
      "Training loss  144.132 \t Recon  137.930 \t KL  6.201 in Step 200\n",
      "Training loss  145.956 \t Recon  139.553 \t KL  6.403 in Step 300\n",
      "Training loss  148.081 \t Recon  141.730 \t KL  6.351 in Step 400\n",
      "Valid loss  148.751 \t Recon  142.329 \t KL  6.421 in epoch 79\n",
      "Model saved\n",
      "Epoch 80\n",
      "Training loss  147.828 \t Recon  141.223 \t KL  6.605 in Step 0\n",
      "Training loss  147.011 \t Recon  140.761 \t KL  6.250 in Step 100\n",
      "Training loss  151.327 \t Recon  145.086 \t KL  6.241 in Step 200\n",
      "Training loss  155.751 \t Recon  149.725 \t KL  6.026 in Step 300\n",
      "Training loss  151.887 \t Recon  145.765 \t KL  6.122 in Step 400\n",
      "Valid loss  148.777 \t Recon  142.350 \t KL  6.427 in epoch 80\n",
      "Epoch 81\n",
      "Training loss  144.427 \t Recon  137.864 \t KL  6.563 in Step 0\n",
      "Training loss  141.594 \t Recon  135.295 \t KL  6.299 in Step 100\n",
      "Training loss  144.673 \t Recon  138.361 \t KL  6.312 in Step 200\n",
      "Training loss  147.802 \t Recon  141.560 \t KL  6.242 in Step 300\n",
      "Training loss  135.315 \t Recon  129.071 \t KL  6.244 in Step 400\n",
      "Valid loss  148.886 \t Recon  142.576 \t KL  6.310 in epoch 81\n",
      "Epoch 82\n",
      "Training loss  147.391 \t Recon  140.949 \t KL  6.442 in Step 0\n",
      "Training loss  147.781 \t Recon  141.287 \t KL  6.495 in Step 100\n",
      "Training loss  147.914 \t Recon  141.363 \t KL  6.551 in Step 200\n",
      "Training loss  141.748 \t Recon  135.352 \t KL  6.395 in Step 300\n",
      "Training loss  140.056 \t Recon  133.643 \t KL  6.414 in Step 400\n",
      "Valid loss  148.835 \t Recon  142.437 \t KL  6.398 in epoch 82\n",
      "Epoch 83\n",
      "Training loss  136.681 \t Recon  129.985 \t KL  6.696 in Step 0\n",
      "Training loss  152.852 \t Recon  146.551 \t KL  6.300 in Step 100\n",
      "Training loss  145.056 \t Recon  138.796 \t KL  6.261 in Step 200\n",
      "Training loss  133.380 \t Recon  126.842 \t KL  6.538 in Step 300\n",
      "Training loss  143.476 \t Recon  137.289 \t KL  6.186 in Step 400\n",
      "Valid loss  149.040 \t Recon  142.671 \t KL  6.369 in epoch 83\n",
      "Epoch 84\n",
      "Training loss  145.496 \t Recon  139.058 \t KL  6.438 in Step 0\n",
      "Training loss  148.889 \t Recon  142.777 \t KL  6.112 in Step 100\n",
      "Training loss  142.857 \t Recon  136.324 \t KL  6.533 in Step 200\n",
      "Training loss  149.212 \t Recon  142.772 \t KL  6.440 in Step 300\n",
      "Training loss  147.512 \t Recon  141.175 \t KL  6.337 in Step 400\n",
      "Valid loss  148.700 \t Recon  142.414 \t KL  6.286 in epoch 84\n",
      "Model saved\n",
      "Epoch 85\n",
      "Training loss  142.835 \t Recon  136.514 \t KL  6.321 in Step 0\n",
      "Training loss  143.012 \t Recon  136.710 \t KL  6.303 in Step 100\n",
      "Training loss  146.252 \t Recon  140.040 \t KL  6.211 in Step 200\n",
      "Training loss  146.090 \t Recon  140.067 \t KL  6.022 in Step 300\n",
      "Training loss  138.743 \t Recon  132.405 \t KL  6.337 in Step 400\n",
      "Valid loss  149.008 \t Recon  142.712 \t KL  6.296 in epoch 85\n",
      "Epoch 86\n",
      "Training loss  142.422 \t Recon  136.120 \t KL  6.302 in Step 0\n",
      "Training loss  142.758 \t Recon  136.176 \t KL  6.582 in Step 100\n",
      "Training loss  146.290 \t Recon  139.997 \t KL  6.293 in Step 200\n",
      "Training loss  140.342 \t Recon  133.993 \t KL  6.349 in Step 300\n",
      "Training loss  148.975 \t Recon  142.545 \t KL  6.430 in Step 400\n",
      "Valid loss  149.061 \t Recon  142.613 \t KL  6.448 in epoch 86\n",
      "Epoch 87\n",
      "Training loss  147.133 \t Recon  140.598 \t KL  6.535 in Step 0\n",
      "Training loss  149.177 \t Recon  142.970 \t KL  6.207 in Step 100\n",
      "Training loss  141.225 \t Recon  135.031 \t KL  6.194 in Step 200\n",
      "Training loss  148.779 \t Recon  142.512 \t KL  6.267 in Step 300\n",
      "Training loss  147.073 \t Recon  140.490 \t KL  6.583 in Step 400\n",
      "Valid loss  148.626 \t Recon  142.330 \t KL  6.296 in epoch 87\n",
      "Model saved\n",
      "Epoch 88\n",
      "Training loss  137.626 \t Recon  131.311 \t KL  6.315 in Step 0\n",
      "Training loss  146.629 \t Recon  140.154 \t KL  6.476 in Step 100\n",
      "Training loss  149.987 \t Recon  143.742 \t KL  6.245 in Step 200\n",
      "Training loss  144.496 \t Recon  138.223 \t KL  6.273 in Step 300\n",
      "Training loss  153.061 \t Recon  146.638 \t KL  6.423 in Step 400\n",
      "Valid loss  148.943 \t Recon  142.577 \t KL  6.367 in epoch 88\n",
      "Epoch 89\n",
      "Training loss  135.484 \t Recon  128.854 \t KL  6.630 in Step 0\n",
      "Training loss  146.704 \t Recon  140.420 \t KL  6.283 in Step 100\n",
      "Training loss  139.941 \t Recon  133.761 \t KL  6.180 in Step 200\n",
      "Training loss  145.814 \t Recon  139.558 \t KL  6.256 in Step 300\n",
      "Training loss  145.500 \t Recon  139.312 \t KL  6.188 in Step 400\n",
      "Valid loss  148.657 \t Recon  142.253 \t KL  6.404 in epoch 89\n",
      "Epoch 90\n",
      "Training loss  145.299 \t Recon  138.818 \t KL  6.481 in Step 0\n",
      "Training loss  143.172 \t Recon  136.991 \t KL  6.181 in Step 100\n",
      "Training loss  134.783 \t Recon  128.240 \t KL  6.543 in Step 200\n",
      "Training loss  137.395 \t Recon  130.983 \t KL  6.411 in Step 300\n",
      "Training loss  145.082 \t Recon  138.517 \t KL  6.565 in Step 400\n",
      "Valid loss  149.043 \t Recon  142.671 \t KL  6.372 in epoch 90\n",
      "Epoch 91\n",
      "Training loss  144.938 \t Recon  138.376 \t KL  6.562 in Step 0\n",
      "Training loss  141.930 \t Recon  135.421 \t KL  6.510 in Step 100\n",
      "Training loss  137.635 \t Recon  131.177 \t KL  6.458 in Step 200\n",
      "Training loss  145.284 \t Recon  138.756 \t KL  6.528 in Step 300\n",
      "Training loss  140.477 \t Recon  134.031 \t KL  6.447 in Step 400\n",
      "Valid loss  148.915 \t Recon  142.513 \t KL  6.402 in epoch 91\n",
      "Epoch 92\n",
      "Training loss  140.597 \t Recon  134.268 \t KL  6.329 in Step 0\n",
      "Training loss  153.623 \t Recon  147.098 \t KL  6.525 in Step 100\n",
      "Training loss  143.631 \t Recon  137.247 \t KL  6.383 in Step 200\n",
      "Training loss  147.123 \t Recon  140.602 \t KL  6.520 in Step 300\n",
      "Training loss  141.121 \t Recon  134.778 \t KL  6.344 in Step 400\n",
      "Valid loss  148.862 \t Recon  142.572 \t KL  6.289 in epoch 92\n",
      "Epoch 93\n",
      "Training loss  142.551 \t Recon  136.192 \t KL  6.359 in Step 0\n",
      "Training loss  149.947 \t Recon  143.842 \t KL  6.105 in Step 100\n",
      "Training loss  145.668 \t Recon  139.230 \t KL  6.438 in Step 200\n",
      "Training loss  147.244 \t Recon  141.029 \t KL  6.215 in Step 300\n",
      "Training loss  143.449 \t Recon  137.037 \t KL  6.412 in Step 400\n",
      "Valid loss  148.681 \t Recon  142.531 \t KL  6.150 in epoch 93\n",
      "Epoch 94\n",
      "Training loss  144.128 \t Recon  137.848 \t KL  6.280 in Step 0\n",
      "Training loss  144.241 \t Recon  137.813 \t KL  6.428 in Step 100\n",
      "Training loss  151.927 \t Recon  145.652 \t KL  6.275 in Step 200\n",
      "Training loss  146.546 \t Recon  140.057 \t KL  6.490 in Step 300\n",
      "Training loss  142.833 \t Recon  136.331 \t KL  6.502 in Step 400\n",
      "Valid loss  148.574 \t Recon  142.240 \t KL  6.334 in epoch 94\n",
      "Model saved\n",
      "Epoch 95\n",
      "Training loss  143.068 \t Recon  136.782 \t KL  6.286 in Step 0\n",
      "Training loss  141.288 \t Recon  134.731 \t KL  6.557 in Step 100\n",
      "Training loss  150.298 \t Recon  143.672 \t KL  6.627 in Step 200\n",
      "Training loss  138.389 \t Recon  131.922 \t KL  6.467 in Step 300\n",
      "Training loss  146.100 \t Recon  139.818 \t KL  6.282 in Step 400\n",
      "Valid loss  148.694 \t Recon  142.377 \t KL  6.317 in epoch 95\n",
      "Epoch 96\n",
      "Training loss  148.555 \t Recon  142.282 \t KL  6.273 in Step 0\n",
      "Training loss  145.111 \t Recon  138.829 \t KL  6.282 in Step 100\n",
      "Training loss  151.712 \t Recon  145.364 \t KL  6.348 in Step 200\n",
      "Training loss  138.077 \t Recon  131.514 \t KL  6.563 in Step 300\n",
      "Training loss  146.784 \t Recon  140.370 \t KL  6.414 in Step 400\n",
      "Valid loss  148.867 \t Recon  142.512 \t KL  6.355 in epoch 96\n",
      "Epoch 97\n",
      "Training loss  140.640 \t Recon  134.173 \t KL  6.467 in Step 0\n",
      "Training loss  149.352 \t Recon  143.073 \t KL  6.279 in Step 100\n",
      "Training loss  146.109 \t Recon  139.758 \t KL  6.351 in Step 200\n",
      "Training loss  147.475 \t Recon  140.990 \t KL  6.485 in Step 300\n",
      "Training loss  147.110 \t Recon  140.627 \t KL  6.483 in Step 400\n",
      "Valid loss  148.594 \t Recon  142.267 \t KL  6.327 in epoch 97\n",
      "Epoch 98\n",
      "Training loss  146.691 \t Recon  140.312 \t KL  6.379 in Step 0\n",
      "Training loss  145.372 \t Recon  138.960 \t KL  6.412 in Step 100\n",
      "Training loss  143.438 \t Recon  136.951 \t KL  6.488 in Step 200\n",
      "Training loss  139.779 \t Recon  133.364 \t KL  6.415 in Step 300\n",
      "Training loss  139.666 \t Recon  133.034 \t KL  6.632 in Step 400\n",
      "Valid loss  148.618 \t Recon  142.355 \t KL  6.263 in epoch 98\n",
      "Epoch 99\n",
      "Training loss  149.570 \t Recon  143.254 \t KL  6.316 in Step 0\n",
      "Training loss  146.651 \t Recon  140.176 \t KL  6.476 in Step 100\n",
      "Training loss  140.686 \t Recon  134.261 \t KL  6.425 in Step 200\n",
      "Training loss  141.969 \t Recon  135.507 \t KL  6.462 in Step 300\n",
      "Training loss  139.390 \t Recon  132.788 \t KL  6.602 in Step 400\n",
      "Valid loss  148.701 \t Recon  142.326 \t KL  6.375 in epoch 99\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e9\n",
    "best_epoch = 0\n",
    "\n",
    "valid_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_num = len(train_loader.dataset)\n",
    "\n",
    "    for idx, (x, _) in enumerate(train_loader):\n",
    "        batch = x.size(0)\n",
    "        x = x.to(device)\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        recon = recon_loss(recon_x, x)\n",
    "        kl = kl_loss(mu, logvar)\n",
    "\n",
    "        loss = recon + kl\n",
    "        train_loss += loss.item()\n",
    "        loss = loss / batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Training loss {loss: .3f} \\t Recon {recon / batch: .3f} \\t KL {kl / batch: .3f} in Step {idx}\")\n",
    "\n",
    "    train_losses.append(train_loss / train_num)\n",
    "\n",
    "    valid_loss = 0.\n",
    "    valid_recon = 0.\n",
    "    valid_kl = 0.\n",
    "    valid_num = len(test_loader.dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, _) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            recon_x, mu, logvar = model(x)\n",
    "            recon = recon_loss(recon_x, x)\n",
    "            kl = kl_loss(mu, logvar)\n",
    "            loss = recon + kl\n",
    "            valid_loss += loss.item()\n",
    "            valid_kl += kl.item()\n",
    "            valid_recon += recon.item()\n",
    "\n",
    "        valid_losses.append(valid_loss / valid_num)\n",
    "\n",
    "        print(\n",
    "            f\"Valid loss {valid_loss / valid_num: .3f} \\t Recon {valid_recon / valid_num: .3f} \\t KL {valid_kl / valid_num: .3f} in epoch {epoch}\")\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "            torch.save(model.state_dict(), 'best_model_mnist')\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(valid_losses, label='Valid')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.53347826e-01 -3.31789801e+00  9.40482699e-01 ...  1.72095439e+00\n",
      "   1.49361428e-01  8.40290871e-01]\n",
      " [-1.57740779e-01  7.52829544e-01 -6.27643371e-01 ... -3.89696013e-01\n",
      "   1.37779274e-01 -5.30537621e-01]\n",
      " [ 1.32615326e-04 -1.32183100e+00  6.23563488e-01 ...  4.23515844e-01\n",
      "   1.24190671e+00 -1.17849920e+00]\n",
      " ...\n",
      " [-1.64260462e-01 -2.44979706e-01 -1.42375277e+00 ...  2.45206412e-01\n",
      "  -2.12376257e-01 -3.73201282e-01]\n",
      " [-4.22483444e-01  6.26407813e-01 -2.72943985e-02 ...  3.55112106e-01\n",
      "   3.70018364e-01 -2.22944635e-01]\n",
      " [-8.48570021e-01 -3.86808302e-01  4.02742532e-01 ... -9.53660235e-01\n",
      "  -1.91641380e+00 -5.00684063e-02]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "data_x, data_y = make_classification(n_samples=1000, n_classes=4, n_features=40, n_informative=4,\n",
    "                                     random_state=0)  # 6\n",
    "print(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0   -0.453348 -3.317898  0.940483 -2.119275 -1.720249 -0.460972 -0.170683   \n",
      "1   -0.157741  0.752830 -0.627643  0.256542 -0.516130 -1.094577  1.816735   \n",
      "2    0.000133 -1.321831  0.623563 -0.263814 -0.106594  0.882225  1.264845   \n",
      "3    0.854828 -0.090137  2.189497 -1.290909  1.457043  0.128231 -0.035935   \n",
      "4   -0.323886  0.386223  0.727295  0.214817 -0.837555 -0.534562  1.516299   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995 -0.578708 -3.081041  1.039982  0.602767  0.722424  0.451383  0.217314   \n",
      "996 -3.417173  0.733903  1.815059  1.110407 -0.669307 -0.234601  2.212747   \n",
      "997 -0.164260 -0.244980 -1.423753 -1.143103 -0.278834  0.365739  0.154387   \n",
      "998 -0.422483  0.626408 -0.027294 -2.031704  0.575582 -2.511324  0.465196   \n",
      "999 -0.848570 -0.386808  0.402743  0.906538  0.736222  2.062678  0.569193   \n",
      "\n",
      "           x8        x9       x10  ...       x31       x32       x33  \\\n",
      "0   -0.561039  0.889168  0.156336  ...  1.009535 -1.421684  0.556060   \n",
      "1   -0.271615 -0.779619 -1.301254  ... -0.995377  1.007977 -0.518925   \n",
      "2   -1.893320  1.167669  0.413200  ...  2.017243 -0.663448  0.111782   \n",
      "3    1.102652 -0.918400 -1.810795  ... -0.081195  1.059843 -0.437716   \n",
      "4    0.794789  0.452201  0.258925  ...  0.068763 -1.187286  0.953680   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995 -0.744483 -1.655114 -0.240754  ... -1.414534  0.056121  0.730647   \n",
      "996 -0.437473  2.479182 -0.896117  ... -0.390552 -0.796359  1.010620   \n",
      "997 -0.245113  1.349769 -2.659474  ...  0.183125  1.495865  1.280396   \n",
      "998  0.278504  3.123987 -0.320092  ...  0.369469 -0.923206  0.515275   \n",
      "999 -0.942311  0.051365  0.316282  ... -0.216600 -0.316146  0.187442   \n",
      "\n",
      "          x34       x35       x36       x37       x38       x39  miss_line  \n",
      "0   -0.895138  1.101165  0.598356 -1.276687  1.720954  0.149361   0.840291  \n",
      "1    1.207170  0.866743 -0.614271 -0.012226 -0.389696  0.137779  -0.530538  \n",
      "2   -1.285246  0.044723 -1.834859  0.818267  0.423516  1.241907  -1.178499  \n",
      "3    0.428993  0.734439 -0.477302  1.496793  0.484241  0.007747  -0.234478  \n",
      "4    0.023366  0.375689  1.112212 -0.609003  0.161999  1.159473  -0.427624  \n",
      "..        ...       ...       ...       ...       ...       ...        ...  \n",
      "995  1.755203  1.686635  1.541738  1.353030  0.140954 -0.471829   0.007751  \n",
      "996 -2.078966  0.408127 -0.699597 -1.143624  1.126030 -0.643245   1.265045  \n",
      "997 -2.030022  1.589750 -1.601515 -0.230006  0.245206 -0.212376  -0.373201  \n",
      "998 -2.542248 -0.407481 -1.767668 -0.242463  0.355112  0.370018  -0.222945  \n",
      "999  0.201229  0.783342  1.087439 -1.541838 -0.953660 -1.916414  -0.050068  \n",
      "\n",
      "[1000 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "data_x = pd.DataFrame(data_x)\n",
    "data_x.columns = [\"x{}\".format(i + 1) for i in range(39)] + [\"miss_line\"]\n",
    "print(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
